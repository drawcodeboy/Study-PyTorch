{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fd48b1",
   "metadata": {},
   "source": [
    "# My Test2\n",
    "## test 1\n",
    "* <code>optimizer.step()</code>에 따라 모델의 가중치를 실제로 뽑아서 변하는지 확인하기\n",
    "    * optimizer에 담긴 parameter가 변하는 것인지 or 실제 모델의 가중치가 변하는 것인지 확인하기 위해서\n",
    "## test 2\n",
    "* 또한 <code>loss.backward()</code>를 할 때 원리가 무엇인지 <code>grad_fn</code>과 관련된 것인지 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "75da8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "849eeb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=16)\n",
    "test_loader = DataLoader(test_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ac57a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(28*28, 512)\n",
    "        self.linear2 = nn.Linear(512, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a828422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 512]         401,920\n",
      "              ReLU-3                  [-1, 512]               0\n",
      "            Linear-4                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 1.55\n",
      "Estimated Total Size (MB): 1.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = MyNet()\n",
    "\n",
    "summary(model, (28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268cdbcd",
   "metadata": {},
   "source": [
    "# Test 1 Conclusion\n",
    "* <code>optimizer.step()</code>을 통해 실제 모델에 갱신되는 것을 확인\n",
    "* 실험이 끝나고서 생각해보면 당연하다. 왜냐하면, 모델을 저장하거나 불러올 때 <b>Model의 오브젝트를 저장</b>하기 때문이다.\n",
    "* 모델의 Parameter를 인자로 넘기기 때문에 오해했다. <b>Call-By-Reference</b>의 형식으로 돌아가는 것으로 유추할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "70daee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataloader, loss_fn, optim):\n",
    "    size = len(dataloader.dataset)\n",
    "    print('[Weight Change Observation]')\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Forward Propagation\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        print('=========================================')\n",
    "        print(f'ORDER: {batch+1}')\n",
    "        print(f'[Before Backward]\\n{model.linear2.weight[0,0]}')\n",
    "        \n",
    "        # Backward Propagation\n",
    "        optim.zero_grad() # init grad\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        print(f'[After Backward]\\n{model.linear2.weight[0,0]}')\n",
    "        \n",
    "        if batch == (10 - 1):\n",
    "            print('=========================================')\n",
    "            print('print 10 times')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "39b8c689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Weight Change Observation]\n",
      "=========================================\n",
      "ORDER: 1\n",
      "[Before Backward]\n",
      "0.005343694239854813\n",
      "[After Backward]\n",
      "0.005184032954275608\n",
      "=========================================\n",
      "ORDER: 2\n",
      "[Before Backward]\n",
      "0.005184032954275608\n",
      "[After Backward]\n",
      "0.00509195402264595\n",
      "=========================================\n",
      "ORDER: 3\n",
      "[Before Backward]\n",
      "0.00509195402264595\n",
      "[After Backward]\n",
      "0.004864421673119068\n",
      "=========================================\n",
      "ORDER: 4\n",
      "[Before Backward]\n",
      "0.004864421673119068\n",
      "[After Backward]\n",
      "0.00480083841830492\n",
      "=========================================\n",
      "ORDER: 5\n",
      "[Before Backward]\n",
      "0.00480083841830492\n",
      "[After Backward]\n",
      "0.00480083841830492\n",
      "=========================================\n",
      "ORDER: 6\n",
      "[Before Backward]\n",
      "0.00480083841830492\n",
      "[After Backward]\n",
      "0.004487521015107632\n",
      "=========================================\n",
      "ORDER: 7\n",
      "[Before Backward]\n",
      "0.004487521015107632\n",
      "[After Backward]\n",
      "0.004345327150076628\n",
      "=========================================\n",
      "ORDER: 8\n",
      "[Before Backward]\n",
      "0.004345327150076628\n",
      "[After Backward]\n",
      "0.004016462713479996\n",
      "=========================================\n",
      "ORDER: 9\n",
      "[Before Backward]\n",
      "0.004016462713479996\n",
      "[After Backward]\n",
      "0.0035967917647212744\n",
      "=========================================\n",
      "ORDER: 10\n",
      "[Before Backward]\n",
      "0.0035967917647212744\n",
      "[After Backward]\n",
      "0.0033088999334722757\n",
      "=========================================\n",
      "print 10 times\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loop(model, train_loader, loss_fn, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8231f5",
   "metadata": {},
   "source": [
    "# Test 2\n",
    "* 각 변수에 대해 <code>.grad_fn</code>를 호출하여 미분을 계산\n",
    "* e.g. $\\frac{\\delta L}{\\delta w}$\n",
    "* 여기서 PyTorch는 w와 L에 관계에 대해 어떻게 알았을까.\n",
    "* <b>연산 그래프</b>를 통해서 알고 있다.\n",
    "* 이 연산 그래프 또한 <code>.grad_fn</code>에서 관리되고 있으며, 동적으로 생성되고, 더 이상 필요하지 않을 때마다 메모리에서 자동으로 삭제된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9a945957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1과 같은 모델이지만 복잡함을 덜어내기 위해 같은 모델 구현\n",
    "# 단, forward에서 grad_fn에 대한 출력문이 추가 되었다.\n",
    "\n",
    "class MyNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(28*28, 512)\n",
    "        self.linear2 = nn.Linear(512, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f'input.grad_fn: {x.grad_fn}')\n",
    "        x = self.flatten(x)\n",
    "        print(f'flatten.grad_fn: {x.grad_fn}')\n",
    "        x = self.linear1(x)\n",
    "        print(f'linear1.grad_fn: {x.grad_fn}')\n",
    "        x = self.relu(x)\n",
    "        print(f'relu.grad_fn: {x.grad_fn}')\n",
    "        x = self.linear2(x)\n",
    "        print(f'linear2.grad_fn: {x.grad_fn}')\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb2e6388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.grad_fn: None\n",
      "flatten.grad_fn: None\n",
      "linear1.grad_fn: <AddmmBackward0 object at 0x0000019353A94AF0>\n",
      "relu.grad_fn: <ReluBackward0 object at 0x0000019353A94AF0>\n",
      "linear2.grad_fn: <AddmmBackward0 object at 0x0000019353A94AF0>\n"
     ]
    }
   ],
   "source": [
    "model2 = MyNet2()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = DataLoader(training_data, batch_size=16)\n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "pred = model2(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
