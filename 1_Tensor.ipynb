{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e3f43b",
   "metadata": {},
   "source": [
    "# Tensor 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d34eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Pytorch, NumPy\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02e8d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로 directly하게 생성하기\n",
    "\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c844ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy 배열로부터 생성\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7675f98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data \n",
      " [[1, 2], [3, 4]] \n",
      "\n",
      "torch.tensor(data) \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "np.array(data) \n",
      " [[1 2]\n",
      " [3 2]] \n",
      "\n",
      "torch.from_numpy(np_array) \n",
      " tensor([[1, 2],\n",
      "        [3, 2]], dtype=torch.int32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x_np와 np_array는 서로 같은 메모리를 공유하고 있는 것을 알 수 있음\n",
    "# from_numpy를 통해서 source인 Numpy 배열,\n",
    "# 이에 대한 리턴 값인 Pytorch 배열\n",
    "# 원본 데이터인 data는 상관이 없음\n",
    "\n",
    "x_np[1][1] = 2\n",
    "\n",
    "print(f'Original Data \\n {data} \\n')\n",
    "print(f'torch.tensor(data) \\n {x_data} \\n')\n",
    "print(f'np.array(data) \\n {np_array} \\n')\n",
    "print(f'torch.from_numpy(np_array) \\n {x_np} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5337ebc",
   "metadata": {},
   "source": [
    "* <b>함수들을 써보고 느낀 점은 type이 기본형으로 <code>torch.float32</code>가 들어가지만</b>\n",
    "* <b>항상 지정해주어야 논리적 오류가 없을 거 같다.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be53610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.7524, 0.2504],\n",
      "        [0.0798, 0.7648]]) \n",
      "\n",
      "Random N Tensor: \n",
      " tensor([[-0.6721,  0.1060],\n",
      "        [-0.1105,  0.7579]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다른 텐서로부터 생성하기\n",
    "# Shape을 그대로 가져간다.\n",
    "# Type도 따로 지정하지 않는다면, 원래 Tensor 그대로 가져간다.\n",
    "\n",
    "# rand = 0~1 사이의 숫자를 균등하게 생성\n",
    "# randn = N(0, 1)인 정규 분포를 따라 추출한 값을 리턴 -> 음수가 나올 수 있다.\n",
    "\n",
    "\n",
    "x_ones = torch.ones_like(x_data, dtype=torch.float32)\n",
    "print(f'Ones Tensor: \\n {x_ones} \\n')\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float32)\n",
    "print(f'Random Tensor: \\n {x_rand} \\n')\n",
    "\n",
    "x_randn = torch.randn_like(x_data, dtype=torch.float32)\n",
    "print(f'Random N Tensor: \\n {x_randn} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a3192ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.8905, 0.7969, 0.2240],\n",
      "        [0.9847, 0.5362, 0.0036]]) \n",
      "\n",
      "RandomN Tensor: \n",
      " tensor([[-0.9059,  0.0425, -1.0959],\n",
      "        [ 0.4511,  0.6399,  0.8163]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor\": \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 무작위(random) or 상수(constant) 값을 사용하기\n",
    "# shape은 텐서의 차원을 나타내는 튜플로, 아래 함수들에서는 출력 텐서의 차원을 결정\n",
    "\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "randn_tensor = torch.randn(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f'Random Tensor: \\n {rand_tensor} \\n')\n",
    "print(f'RandomN Tensor: \\n {randn_tensor} \\n')\n",
    "print(f'Ones Tensor: \\n {ones_tensor} \\n')\n",
    "print(f'Zeros Tensor: \\n {zeros_tensor} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feda113",
   "metadata": {},
   "source": [
    "# Tensor의 Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1c19d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is store on : cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4, dtype=torch.float32)\n",
    "\n",
    "print(f'Shape of tensor: {tensor.shape}')\n",
    "print(f'Datatype of tensor: {tensor.dtype}')\n",
    "print(f'Device tensor is store on : {tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b207f",
   "metadata": {},
   "source": [
    "# Tensor의 Operation\n",
    "* Transposing, indexing, slicing, 수학 계산, 선형 대수, random sampling, 100가지 이상의 Tensor Operation들은 GPU에서 실행시킬 수 있다.\n",
    "* Colab의 경우 Edit > Notebook Settings에서 GPU 할당 가능\n",
    "* 기본적으로 텐서는 CPU에 생성되어서 <code>.to</code> 메소드를 사용하면 (GPU의 가용성(availability)을 확인한 뒤) GPU로 텐서를 이동할 수 있다.\n",
    "* 장치들 간에 큰 텐서들을 복사하는 것은 시간과 메모리 측면에서 비용이 많이 든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0a94297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d037506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last Column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# NumPy식의 표준 인덱싱과 슬라이싱\n",
    "\n",
    "tensor = torch.ones(4, 4, dtype=torch.float32)\n",
    "print(f'First row: {tensor[0]}')\n",
    "print(f'First column: {tensor[:, 0]}')\n",
    "print(f'Last Column: {tensor[..., -1]}')\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74c8451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat, 텐서합치기\n",
    "\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "t2 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56ce1fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Matrix Multiplication>\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n",
      "<Element Product>\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic operations\n",
    "\n",
    "# matrix multiplication - y1, y2, y3 모두 같은 값\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "# element-wise product - z1, z2, z3 모두 같은 값\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "print('<Matrix Multiplication>')\n",
    "print(y1, y2, y3, sep='\\n')\n",
    "\n",
    "print()\n",
    "\n",
    "print('<Element Product>')\n",
    "print(z1, z2, z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29e01ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.) <class 'torch.Tensor'> \n",
      " 12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# Single-element tensor의 aggregate\n",
    "\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item() # scalar value 리턴\n",
    "print(agg, type(agg), '\\n', agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c424faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# in-place 연산\n",
    "\n",
    "print(f'{tensor} \\n')\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6cc81",
   "metadata": {},
   "source": [
    "# Numpy 변환 (Bridge)\n",
    "* CPU 상의 텐서와 NumPy 배열은 메모리 공간을 공유하기 때문에, 하나를 변경하면 다른 하나도 변경된다.\n",
    "* 그럼 만약에 텐서가 GPU에 있고, NumPy가 CPU에 있다면? -> 이런 경우에는 공유 안 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cb2f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False [5 2 3] tensor([5, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3])\n",
    "tensor = torch.from_numpy(arr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    \n",
    "arr[0] = 5\n",
    "print(torch.cuda.is_available(), arr, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4949b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Tensor를 NumPy 배열로 변환하기\n",
    "\n",
    "t = torch.ones(5)\n",
    "print(f't: {t}')\n",
    "n = t.numpy()\n",
    "print(f'n: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5bf5943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# CPU 상 메모리 공유\n",
    "\n",
    "t.add_(1)\n",
    "print(f't: {t}')\n",
    "print(f'n: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c256da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy 배열을 텐서로 변환하기\n",
    "\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ae33653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# NumPy 배열에서 변경되어도 메모리 공유된다.\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(f't: {t}')\n",
    "print(f'n: {n}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
